<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="随感，遥感，机器学习....想到什么写什么"><title>tensorflow-二十二弹 | 吴蔚</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">tensorflow-二十二弹</h1><a id="logo" href="/.">吴蔚</a><p class="description">生命不息，折腾不止！</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">tensorflow-二十二弹</h1><div class="post-meta">Jan 21, 2018<span> | </span><span class="category"><a href="/categories/学习/">学习</a></span></div><a class="disqus-comment-count" href="/2018/01/21/tensorflow-二十二弹/#vcomment"><span class="valine-comment-count" data-xid="/2018/01/21/tensorflow-二十二弹/"></span><span> 条评论</span></a><div class="post-content"><p>上次我们谈到了Q-Learning算法，实际上就是构建一个Q表的过程，那么我们怎么将深度网络和Q-Learning算法相结合，这个就是我们这篇文章会提到的问题．我们依旧拿上次的那个FlappyBird的例子来进行说明，在这个例子中如果我们通过Q-Learning算法进行处理，那么实际上Q表就是整个影像范围的所有像素，而每一个像素又存着0-255的像素值的情况，即使没有0-255的像素值这么多，整个Q表也是一个极大的值，在这样的情况下求出Q表中的每一个值明显不现实，需要减小Q表的规模，在这样的情况下我们就引入了深度学习算法．<br>在上一篇中我们提到过实际上我们也不一定需要一个Q表，我们只需要拟合一个Q值函数就可以了，也就是能够估计Q表的分布，就能够进行计算了，简单的来说我们的模型为：<br><img src="http://blogimage-1251632003.cosgz.myqcloud.com/DQN-%E7%8A%B6%E6%80%81%E8%AF%B4%E6%98%8E.png"><br>在传统的Q-Learning算法中，其中Q表为一个表，而对于某些任务，由于Q表过大，不宜用一个表来进行展示，因此我们将Q表转换为一个能够根据输入解算出对应所有action的reward的所有值的一个网络，因此我们的整个算法过程就分为两个部分：１.求解Q网络的过程;2.根据Q网络进行判断的过程；好了，废话也不多说了，我们直接解析整个代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"game/"</span>)</span><br><span class="line"><span class="keyword">import</span> wrapped_flappy_bird <span class="keyword">as</span> game</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line">GAME = <span class="string">'bird'</span> <span class="comment"># the name of the game being played for log files</span></span><br><span class="line">ACTIONS = <span class="number">2</span> <span class="comment"># number of valid actions</span></span><br><span class="line">GAMMA = <span class="number">0.99</span> <span class="comment"># decay rate of past observations</span></span><br><span class="line">OBSERVE = <span class="number">100000.</span> <span class="comment"># timesteps to observe before training</span></span><br><span class="line">EXPLORE = <span class="number">2000000.</span> <span class="comment"># frames over which to anneal epsilon</span></span><br><span class="line">FINAL_EPSILON = <span class="number">0.0001</span> <span class="comment"># final value of epsilon</span></span><br><span class="line">INITIAL_EPSILON = <span class="number">0.0001</span> <span class="comment"># starting value of epsilon</span></span><br><span class="line">REPLAY_MEMORY = <span class="number">50000</span> <span class="comment"># number of previous transitions to remember</span></span><br><span class="line">BATCH = <span class="number">32</span> <span class="comment"># size of minibatch</span></span><br><span class="line">FRAME_PER_ACTION = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev = <span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.01</span>, shape = shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W, stride)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides = [<span class="number">1</span>, stride, stride, <span class="number">1</span>], padding = <span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding = <span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createNetwork</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># network weights</span></span><br><span class="line">    W_conv1 = weight_variable([<span class="number">8</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">32</span>])</span><br><span class="line">    b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line">    W_conv2 = weight_variable([<span class="number">4</span>, <span class="number">4</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">    W_conv3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv3 = bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">    W_fc1 = weight_variable([<span class="number">1600</span>, <span class="number">512</span>])</span><br><span class="line">    b_fc1 = bias_variable([<span class="number">512</span>])</span><br><span class="line"></span><br><span class="line">    W_fc2 = weight_variable([<span class="number">512</span>, ACTIONS])</span><br><span class="line">    b_fc2 = bias_variable([ACTIONS])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input layer</span></span><br><span class="line">    s = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">80</span>, <span class="number">80</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># hidden layers</span></span><br><span class="line">    h_conv1 = tf.nn.relu(conv2d(s, W_conv1, <span class="number">4</span>) + b_conv1)</span><br><span class="line">    h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, <span class="number">2</span>) + b_conv2)</span><br><span class="line">    <span class="comment">#h_pool2 = max_pool_2x2(h_conv2)</span></span><br><span class="line"></span><br><span class="line">    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, <span class="number">1</span>) + b_conv3)</span><br><span class="line">    <span class="comment">#h_pool3 = max_pool_2x2(h_conv3)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#h_pool3_flat = tf.reshape(h_pool3, [-1, 256])</span></span><br><span class="line">    h_conv3_flat = tf.reshape(h_conv3, [<span class="number">-1</span>, <span class="number">1600</span>])</span><br><span class="line"></span><br><span class="line">    h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># readout layer</span></span><br><span class="line">    readout = tf.matmul(h_fc1, W_fc2) + b_fc2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s, readout, h_fc1</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNetwork</span><span class="params">(s, readout, h_fc1, sess)</span>:</span></span><br><span class="line">    <span class="comment"># define the cost function</span></span><br><span class="line">    a = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, ACTIONS])</span><br><span class="line">    y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>])</span><br><span class="line">    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=<span class="number">1</span>)</span><br><span class="line">    cost = tf.reduce_mean(tf.square(y - readout_action))</span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-6</span>).minimize(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># open up a game state to communicate with emulator</span></span><br><span class="line">    game_state = game.GameState()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store the previous observations in replay memory</span></span><br><span class="line">    D = deque()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># printing</span></span><br><span class="line">    a_file = open(<span class="string">"logs_"</span> + GAME + <span class="string">"/readout.txt"</span>, <span class="string">'w'</span>)</span><br><span class="line">    h_file = open(<span class="string">"logs_"</span> + GAME + <span class="string">"/hidden.txt"</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the first state by doing nothing and preprocess the image to 80x80x4</span></span><br><span class="line">    do_nothing = np.zeros(ACTIONS)</span><br><span class="line">    do_nothing[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    x_t, r_0, terminal = game_state.frame_step(do_nothing)</span><br><span class="line">    x_t = cv2.cvtColor(cv2.resize(x_t, (<span class="number">80</span>, <span class="number">80</span>)), cv2.COLOR_BGR2GRAY)</span><br><span class="line">    ret, x_t = cv2.threshold(x_t,<span class="number">1</span>,<span class="number">255</span>,cv2.THRESH_BINARY)</span><br><span class="line">    s_t = np.stack((x_t, x_t, x_t, x_t), axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># saving and loading networks</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    sess.run(tf.initialize_all_variables())</span><br><span class="line">    checkpoint = tf.train.get_checkpoint_state(<span class="string">"saved_networks"</span>)</span><br><span class="line">    <span class="keyword">if</span> checkpoint <span class="keyword">and</span> checkpoint.model_checkpoint_path:</span><br><span class="line">        saver.restore(sess, checkpoint.model_checkpoint_path)</span><br><span class="line">        print(<span class="string">"Successfully loaded:"</span>, checkpoint.model_checkpoint_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"Could not find old network weights"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># start training</span></span><br><span class="line">    epsilon = INITIAL_EPSILON</span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="string">"flappy bird"</span> != <span class="string">"angry bird"</span>:</span><br><span class="line">        <span class="comment"># choose an action epsilon greedily</span></span><br><span class="line">        readout_t = readout.eval(feed_dict=&#123;s : [s_t]&#125;)[<span class="number">0</span>]</span><br><span class="line">        a_t = np.zeros([ACTIONS])</span><br><span class="line">        action_index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> t % FRAME_PER_ACTION == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> random.random() &lt;= epsilon:</span><br><span class="line">                print(<span class="string">"----------Random Action----------"</span>)</span><br><span class="line">                action_index = random.randrange(ACTIONS)</span><br><span class="line">                a_t[random.randrange(ACTIONS)] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                action_index = np.argmax(readout_t)</span><br><span class="line">                a_t[action_index] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            a_t[<span class="number">0</span>] = <span class="number">1</span> <span class="comment"># do nothing</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># scale down epsilon</span></span><br><span class="line">        <span class="keyword">if</span> epsilon &gt; FINAL_EPSILON <span class="keyword">and</span> t &gt; OBSERVE:</span><br><span class="line">            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE</span><br><span class="line"></span><br><span class="line">        <span class="comment"># run the selected action and observe next state and reward</span></span><br><span class="line">        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)</span><br><span class="line">        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (<span class="number">80</span>, <span class="number">80</span>)), cv2.COLOR_BGR2GRAY)</span><br><span class="line">        ret, x_t1 = cv2.threshold(x_t1, <span class="number">1</span>, <span class="number">255</span>, cv2.THRESH_BINARY)</span><br><span class="line">        x_t1 = np.reshape(x_t1, (<span class="number">80</span>, <span class="number">80</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment">#s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)</span></span><br><span class="line">        s_t1 = np.append(x_t1, s_t[:, :, :<span class="number">3</span>], axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># store the transition in D</span></span><br><span class="line">        D.append((s_t, a_t, r_t, s_t1, terminal))</span><br><span class="line">        <span class="keyword">if</span> len(D) &gt; REPLAY_MEMORY:</span><br><span class="line">            D.popleft()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># only train if done observing</span></span><br><span class="line">        <span class="keyword">if</span> t &gt; OBSERVE:</span><br><span class="line">            <span class="comment"># sample a minibatch to train on</span></span><br><span class="line">            minibatch = random.sample(D, BATCH)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># get the batch variables</span></span><br><span class="line">            s_j_batch = [d[<span class="number">0</span>] <span class="keyword">for</span> d <span class="keyword">in</span> minibatch]</span><br><span class="line">            a_batch = [d[<span class="number">1</span>] <span class="keyword">for</span> d <span class="keyword">in</span> minibatch]</span><br><span class="line">            r_batch = [d[<span class="number">2</span>] <span class="keyword">for</span> d <span class="keyword">in</span> minibatch]</span><br><span class="line">            s_j1_batch = [d[<span class="number">3</span>] <span class="keyword">for</span> d <span class="keyword">in</span> minibatch]</span><br><span class="line"></span><br><span class="line">            y_batch = []</span><br><span class="line">            readout_j1_batch = readout.eval(feed_dict = &#123;s : s_j1_batch&#125;)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(minibatch)):</span><br><span class="line">                terminal = minibatch[i][<span class="number">4</span>]</span><br><span class="line">                <span class="comment"># if terminal, only equals reward</span></span><br><span class="line">                <span class="keyword">if</span> terminal:</span><br><span class="line">                    y_batch.append(r_batch[i])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># perform gradient step</span></span><br><span class="line">            train_step.run(feed_dict = &#123;</span><br><span class="line">                y : y_batch,</span><br><span class="line">                a : a_batch,</span><br><span class="line">                s : s_j_batch&#125;</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># update the old values</span></span><br><span class="line">        s_t = s_t1</span><br><span class="line">        t += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># save progress every 10000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> t % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            saver.save(sess, <span class="string">'saved_networks/'</span> + GAME + <span class="string">'-dqn'</span>, global_step = t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print info</span></span><br><span class="line">        state = <span class="string">""</span></span><br><span class="line">        <span class="keyword">if</span> t &lt;= OBSERVE:</span><br><span class="line">            state = <span class="string">"observe"</span></span><br><span class="line">        <span class="keyword">elif</span> t &gt; OBSERVE <span class="keyword">and</span> t &lt;= OBSERVE + EXPLORE:</span><br><span class="line">            state = <span class="string">"explore"</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            state = <span class="string">"train"</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"TIMESTEP"</span>, t, <span class="string">"/ STATE"</span>, state, \</span><br><span class="line">            <span class="string">"/ EPSILON"</span>, epsilon, <span class="string">"/ ACTION"</span>, action_index, <span class="string">"/ REWARD"</span>, r_t, \</span><br><span class="line">            <span class="string">"/ Q_MAX %e"</span> % np.max(readout_t))</span><br><span class="line">        <span class="comment"># write info to files</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        if t % 10000 &lt;= 100:</span></span><br><span class="line"><span class="string">            a_file.write(",".join([str(x) for x in readout_t]) + '\n')</span></span><br><span class="line"><span class="string">            h_file.write(",".join([str(x) for x in h_fc1.eval(feed_dict=&#123;s:[s_t]&#125;)[0]]) + '\n')</span></span><br><span class="line"><span class="string">            cv2.imwrite("logs_tetris/frame" + str(t) + ".png", x_t1)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">playGame</span><span class="params">()</span>:</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    s, readout, h_fc1 = createNetwork()</span><br><span class="line">    trainNetwork(s, readout, h_fc1, sess)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    playGame()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>好了,我们下面来分析以上代码，首先是头文件的引入和模块的定义，这个对于我们来说都没有特别的，在这里就不进行详细的介绍了，实际上就说明一下那个Action，由于在游戏中我们只有点击屏幕和不点击两个动作，因此Action也只有两个，我们先整体了解代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W, stride)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createNetwork</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#训练网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNetwork</span><span class="params">(s, readout, h_fc1, sess)</span>:</span></span><br></pre></td></tr></table></figure></p>
<p>上面这几个过程都是深度卷积网络要干的事情，包括定义权重，定义偏置，定义卷积池化，以及创建网络的过程，这个过程在介绍深度卷积神经网络的时候就介绍过，在这里就不进行更加详细的介绍了，我们重点介绍的是这个训练的过程，实际上这个训练的过程也就是不停的对游戏进行play的过程，首先是初始化输入和输出，在tensorflow中使用palceholder来对变量进行初始化，然后定义行为，实际就是将行为输入我们的网络，然后得到下一个行为，而将我们通过网络得到的行为和我们定义的下一个行为的差值作为代价函数，通过这种方式进行训练；首先我们获取游戏的状态，game_state,然后将游戏的状态保存在一个队列中，定义两个动作，中间的过程就是一些预处理的过程，包括什么游戏初始化，初始化过程中采用[0,1]这个状态进行初始化，然后是一些预处理和读取checkpoint的过程，因为在训练过程中防止无法一次得到最佳结果，通过记录中间过程的方式可以断点重新计算；<br>上面介绍了整个训练的准备过程，下面的重点就是介绍整个训练过程，整个训练过程从代码中我们看出，再每一论游戏的过程中，有两个选择策略，贪婪选择和随机选择，实际上这里存在一个折扣未来奖励的过程，因为我们的选取的当前最好的结果不一定是未来最好的结果，因此这里又一个epsilon的比例，在我们选择好了折扣未来奖励后我们有一定的机率选择随机或者当前最大值，更新epsilon，并根据我们的选取动作执行我们的行为，这样我们得到了下一个状态和当前状态的reward值，将状态存储在队列中，在队列中随机取状态，得到ｙ值，然后将得到的ｙ值和通过Q网络计算的y值进行比较对网络进行调整，每经过一定的步长之后对Q网络的参数进行更新，这样就完成了一次训练过程，然后不停的进行游戏，训练得到最佳结果；</p>
</div><div class="tags"><a href="/tags/tensorflow学习/">tensorflow学习</a></div><div class="post-nav"><a class="pre" href="/2018/01/29/激光点云与影像的颜色映射/">激光点云与影像的颜色映射</a><a class="next" href="/2018/01/21/理想主义者的献身/">理想主义者的献身</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'true' == true ? true : false;
var verify = 'true' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'uHNJtTxLkcGm3VRtuRGk53hb-gzGzoHsz',
  appKey:'h8s1QFO2dLQ62H0axpKB6WnD',
  placeholder:'说点啥吧...',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"><input type="hidden" name="si" value="http://www.wuweiblog.com"><input name="tn" type="hidden" value="bds"><input name="cl" type="hidden" value="3"><input name="ct" type="hidden" value="2097152"><input name="s" type="hidden" value="on"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/书评/">书评</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/图像处理/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/小说/">小说</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/影评/">影评</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/游戏/">游戏</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/点云处理/">点云处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随感/">随感</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/ArcGIS，前端开发/" style="font-size: 15px;">ArcGIS，前端开发</a> <a href="/tags/ArcGIS环境配置/" style="font-size: 15px;">ArcGIS环境配置</a> <a href="/tags/ArcGIS开发/" style="font-size: 15px;">ArcGIS开发</a> <a href="/tags/开发/" style="font-size: 15px;">开发</a> <a href="/tags/linux-学习/" style="font-size: 15px;">linux 学习</a> <a href="/tags/学习/" style="font-size: 15px;">学习</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/图像处理数学原理/" style="font-size: 15px;">图像处理数学原理</a> <a href="/tags/效率/" style="font-size: 15px;">效率</a> <a href="/tags/系统架构-学习/" style="font-size: 15px;">系统架构 学习</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/机器学习，图像处理/" style="font-size: 15px;">机器学习，图像处理</a> <a href="/tags/tensorflow学习/" style="font-size: 15px;">tensorflow学习</a> <a href="/tags/随感/" style="font-size: 15px;">随感</a> <a href="/tags/Mary-and-Max，影评/" style="font-size: 15px;">Mary and Max，影评</a> <a href="/tags/沉默的大多数，书评/" style="font-size: 15px;">沉默的大多数，书评</a> <a href="/tags/断舍离，书评/" style="font-size: 15px;">断舍离，书评</a> <a href="/tags/雪中悍刀行-书评/" style="font-size: 15px;">雪中悍刀行,书评</a> <a href="/tags/呼兰河传-书评/" style="font-size: 15px;">呼兰河传,书评</a> <a href="/tags/将夜-书评/" style="font-size: 15px;">将夜,书评</a> <a href="/tags/中国经济2019-时间的答案/" style="font-size: 15px;">中国经济2019,时间的答案</a> <a href="/tags/未来简史-书评/" style="font-size: 15px;">未来简史,书评</a> <a href="/tags/图像处理/" style="font-size: 15px;">图像处理</a> <a href="/tags/潜规则-书评/" style="font-size: 15px;">潜规则,书评</a> <a href="/tags/你好疯子，影评/" style="font-size: 15px;">你好疯子，影评</a> <a href="/tags/熊镇-书评/" style="font-size: 15px;">熊镇,书评</a> <a href="/tags/openMVG-openMVS学习/" style="font-size: 15px;">openMVG openMVS学习</a> <a href="/tags/喜剧之王，影评/" style="font-size: 15px;">喜剧之王，影评</a> <a href="/tags/The-Legend-of-1900/" style="font-size: 15px;">The Legend of 1900</a> <a href="/tags/随感-摄影测量/" style="font-size: 15px;">随感-摄影测量</a> <a href="/tags/随感，毕业/" style="font-size: 15px;">随感，毕业</a> <a href="/tags/校正方法，控制点，光束法平差/" style="font-size: 15px;">校正方法，控制点，光束法平差</a> <a href="/tags/linux学习/" style="font-size: 15px;">linux学习</a> <a href="/tags/月亮与六便士/" style="font-size: 15px;">月亮与六便士</a> <a href="/tags/电影十二公民/" style="font-size: 15px;">电影十二公民</a> <a href="/tags/随感，死亡/" style="font-size: 15px;">随感，死亡</a> <a href="/tags/小说/" style="font-size: 15px;">小说</a> <a href="/tags/V字仇杀队-浪潮，影评/" style="font-size: 15px;">V字仇杀队,浪潮，影评</a> <a href="/tags/随感－代码重构/" style="font-size: 15px;">随感－代码重构</a> <a href="/tags/遗愿清单，影评/" style="font-size: 15px;">遗愿清单，影评</a> <a href="/tags/指环王，这个杀手不太冷/" style="font-size: 15px;">指环王，这个杀手不太冷</a> <a href="/tags/R-学习/" style="font-size: 15px;">R 学习</a> <a href="/tags/点云处理/" style="font-size: 15px;">点云处理</a> <a href="/tags/爱乐之城，影评/" style="font-size: 15px;">爱乐之城，影评</a> <a href="/tags/狗子日记/" style="font-size: 15px;">狗子日记</a> <a href="/tags/一个叫欧维的男人决定去死，书评/" style="font-size: 15px;">一个叫欧维的男人决定去死，书评</a> <a href="/tags/星际穿越，影评/" style="font-size: 15px;">星际穿越，影评</a> <a href="/tags/图像处理的数学原理/" style="font-size: 15px;">图像处理的数学原理</a> <a href="/tags/社交网络，影评/" style="font-size: 15px;">社交网络，影评</a> <a href="/tags/共产党宣言/" style="font-size: 15px;">共产党宣言</a> <a href="/tags/秒速五厘米/" style="font-size: 15px;">秒速五厘米</a> <a href="/tags/海涛之声，影评/" style="font-size: 15px;">海涛之声，影评</a> <a href="/tags/白日梦想家，影评/" style="font-size: 15px;">白日梦想家，影评</a> <a href="/tags/饥荒/" style="font-size: 15px;">饥荒</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/01/31/社会与资本/">社会与资本</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/25/有那么一瞬间/">有那么一瞬间</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/06/死亡记录/">死亡记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/06/不读书-九/">不读书(九)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/挣扎/">挣扎</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/02/点到三维空间直线距离计算以向量方式计算/">点到三维空间直线距离计算以向量方式计算</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/30/ArcGIS-结合-WebGL动态渲染1/">ArcGIS 结合 WebGL动态渲染1</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/30/狗子日记十一月四号/">狗子日记十一月四号</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/18/点云处理总结/">点云处理总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/13/关于方向/">关于方向</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/RemoteSensingFrank" title="Github" target="_blank">Github</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">吴蔚.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>