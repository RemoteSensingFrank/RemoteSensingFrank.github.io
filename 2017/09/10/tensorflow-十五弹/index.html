<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="随感，遥感，机器学习....想到什么写什么"><title>tensorflow-十五弹 | 吴蔚</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">tensorflow-十五弹</h1><a id="logo" href="/.">吴蔚</a><p class="description">生命不息，折腾不止！</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">tensorflow-十五弹</h1><div class="post-meta">Sep 10, 2017<span> | </span><span class="category"><a href="/categories/学习/">学习</a></span></div><a class="disqus-comment-count" href="/2017/09/10/tensorflow-十五弹/#vcomment"><span class="valine-comment-count" data-xid="/2017/09/10/tensorflow-十五弹/"></span><span> 条评论</span></a><div class="post-content"><p>介绍了很多关于神经网络的理论的东西，当然理论的介绍还差得远，但是再介绍理论上的东西未免显得太无聊了，所以我们中间穿插一些关于tensorflow的小技巧的介绍以及相关算法的简单说明：</p>
<ul>
<li>自定义损失函数：在神经网络的学习过程中我们使用损失函数去判别学习好坏，实际上损失函数的定义有很多种，tensorflow中也内置了很多种类的损失函数，一般来说这些损失函数已经能够训练出比较好的效果了，但是对于实际问题我们常常会有着不同的要求，在此情况下所需的损失函数也会有所差异，在tensorflow中可以使用自定义的损失函数来进行约束，使得求解结果满足要求．示例代码如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</span><br><span class="line"></span><br><span class="line"><span class="comment">#y=wx</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">x = tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>),name=<span class="string">'x-input'</span>)</span><br><span class="line">y_= tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">1</span>),name=<span class="string">'y-input'</span>)</span><br><span class="line">w1= tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x,w1);</span><br><span class="line"></span><br><span class="line"><span class="comment">#loss</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">loss_less = 10</span></span><br><span class="line"><span class="string">loss_more = 1</span></span><br><span class="line"><span class="string">loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * loss_more, (y_ - y) * loss_less))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">loss_less = 1</span></span><br><span class="line"><span class="string">loss_more = 10</span></span><br><span class="line"><span class="string">loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * loss_more, (y_ - y) * loss_less))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">loss = tf.losses.mean_squared_error(y,y_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#data</span></span><br><span class="line">rdm = RandomState(<span class="number">1</span>)</span><br><span class="line">X = rdm.rand(<span class="number">128</span>,<span class="number">2</span>)</span><br><span class="line">Y = [[x1+x2+(rdm.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment">#train</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    STEPS = <span class="number">5000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*batch_size)%<span class="number">128</span></span><br><span class="line">        end   = (i*batch_size)%<span class="number">128</span>+batch_size</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x:X[start:end],y_:Y[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span>(i%<span class="number">1000</span>==<span class="number">0</span>):</span><br><span class="line">            print(<span class="string">'After %d train step(s) ,w1 is: '</span>%(i))</span><br><span class="line">            <span class="keyword">print</span> sess.run(w1), <span class="string">'\n'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'finnal w1 is: \n'</span>,sess.run(w1)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>以上代码很简单，具体过程我们就不进行详细分析，直接看损失函数定义的位置（被多行注释的位置）从损失函数定义我们可以看到定义了三种损失函数，第一种为预测值小于观测值时误差具有较大的权重，此时拟合结果偏向于观测值较大的方向，第二种为预测值大于观测值时误差具有较大的权重，此时结果偏向于观测值较小的方向，最后是均方差．从以上的误差定义可以看出我们可以使用自定义的误差函数对误差权重进行调整使得结果朝着我们预期的方向发展．</p>
<ul>
<li>学习率的设置：实际上在学习过程中学习率是一个很重要的量，学习率设置得越小则每一步学习的步长越小，学习结果越精确，但是计算效率也更低，而学习率设置得值比较大，则学习效率较高但是学习结果越粗糙，因此设置合适的学习率对求解有着重要的影响，下面我们看看在tensorflow中如何对学习率进行设置：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">TRAIN_STEPs=10</span></span><br><span class="line"><span class="string">LEARN_RATE = 1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">x = tf.Variable(tf.constant(5, dtype=tf.float32), name="x")</span></span><br><span class="line"><span class="string">y = tf.square(x)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">train_op = tf.train.GradientDescentOptimizer(LEARN_RATE).minimize(y)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">with tf.Session() as sess:</span></span><br><span class="line"><span class="string">    sess.run(tf.global_variables_initializer())</span></span><br><span class="line"><span class="string">    for i in range(TRAIN_STEPs):</span></span><br><span class="line"><span class="string">        sess.run(train_op)</span></span><br><span class="line"><span class="string">        x_value = sess.run(x)</span></span><br><span class="line"><span class="string">        print "After %s iteration(s): x%s is %f."% (i+1, i+1, x_value)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">#'''</span></span><br><span class="line">TRAINING_STEPS = <span class="number">100</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>)</span><br><span class="line">LEARNING_RATE = tf.train.exponential_decay(<span class="number">0.1</span>, global_step, <span class="number">1</span>, <span class="number">0.96</span>, staircase=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">x = tf.Variable(tf.constant(<span class="number">5</span>, dtype=tf.float32), name=<span class="string">"x"</span>)</span><br><span class="line">y = tf.square(x)</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y, global_step=global_step)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">        sess.run(train_op)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            LEARNING_RATE_value = sess.run(LEARNING_RATE)</span><br><span class="line">            x_value = sess.run(x)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"After %s iteration(s): x%s is %f, learning rate is %f."</span>% (i+<span class="number">1</span>, i+<span class="number">1</span>, x_value, LEARNING_RATE_value)</span><br><span class="line"><span class="comment">#'''</span></span><br></pre></td></tr></table></figure>
<p>上述代码有两种学习率设置的方式，第一种为固定一个学习率，第二种为随着迭代数的增加学习率也在随着增加，设置第一种学习方式比较简单，第二种学习方式在梯度下降算法中使用得很多，随着迭代次数的增加，计算结果也越接近真实值，此时需要降低学习率以便于能够进行更加精确的学习，而且随着学习结果越接近真值就需要更加精确的设置学习率以避免计算中出现震荡的现象．</p>
<ul>
<li>正则化，这个问题想必搞学习算法的人都会有遇到，简单的来说求解求解 $argmin{||y_i-f(x;w)||^2+\lambda\Omega(w)}$的问题，其中 $||y_i-f(x;w)||^2$为损失函数，当然损失函数的定义有很多种，而 $\lambda\Omega(w)$ 为正则化项，为什么要进行正则化，这是由于在计算过程中为了使得计算结果符合<a href="https://zh.wikipedia.org/zh-hans/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80" target="_blank" rel="noopener">奥卡姆剃刀法则</a>,使得采用最简单的形式表示$y$与$x$之间的关系，因此采用正则项进行约束，常用的正则项包括$L0,L1,L2$ 其中$L0$ 又称为稀疏约束，说明约束项为使得参数中非０项最少．在tensorflow中可以很方便的定义正则项：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">data = []</span><br><span class="line">label = []</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">150</span>):</span><br><span class="line">    x1 = np.random.uniform(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    x2 = np.random.uniform(<span class="number">0</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> x1**<span class="number">2</span> + x2**<span class="number">2</span> &lt;= <span class="number">1</span>:</span><br><span class="line">        data.append([np.random.normal(x1, <span class="number">0.1</span>),np.random.normal(x2,<span class="number">0.1</span>)])</span><br><span class="line">        label.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data.append([np.random.normal(x1, <span class="number">0.1</span>), np.random.normal(x2, <span class="number">0.1</span>)])</span><br><span class="line">        label.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = np.hstack(data).reshape(<span class="number">-1</span>,<span class="number">2</span>)</span><br><span class="line">label = np.hstack(label).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#plt.scatter(data[:,0], data[:,1], c=label,</span></span><br><span class="line"><span class="comment">#           cmap="RdBu", vmin=-.2, vmax=1.2, edgecolor="white")</span></span><br><span class="line"><span class="comment">#plt.show()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, lambda1)</span>:</span></span><br><span class="line">    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(lambda1)(var))</span><br><span class="line">    <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line">sample_size = len(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer_dimension = [<span class="number">2</span>,<span class="number">10</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">1</span>]</span><br><span class="line">n_layers = len(layer_dimension)</span><br><span class="line"></span><br><span class="line">cur_layer = x</span><br><span class="line">in_dimension = layer_dimension[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># full connected net</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">    out_dimension = layer_dimension[i]</span><br><span class="line">    weight = get_weight([in_dimension, out_dimension], <span class="number">0.003</span>)</span><br><span class="line">    bias = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[out_dimension]))</span><br><span class="line">    cur_layer = tf.nn.elu(tf.matmul(cur_layer, weight) + bias)</span><br><span class="line">    in_dimension = layer_dimension[i]</span><br><span class="line"></span><br><span class="line">y= cur_layer</span><br><span class="line"></span><br><span class="line"><span class="comment"># def loss func</span></span><br><span class="line">mse_loss = tf.reduce_sum(tf.pow(y_ - y, <span class="number">2</span>)) / sample_size</span><br><span class="line">tf.add_to_collection(<span class="string">'losses'</span>, mse_loss)</span><br><span class="line">loss = tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#train</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">train_op = tf.train.AdamOptimizer(0.001).minimize(mse_loss)</span></span><br><span class="line"><span class="string">TRAINING_STEPS = 40000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">with tf.Session() as sess:</span></span><br><span class="line"><span class="string">    tf.global_variables_initializer().run()</span></span><br><span class="line"><span class="string">    for i in range(TRAINING_STEPS):</span></span><br><span class="line"><span class="string">        sess.run(train_op, feed_dict=&#123;x: data, y_: label&#125;)</span></span><br><span class="line"><span class="string">        if i % 2000 == 0:</span></span><br><span class="line"><span class="string">            print("After %d steps, mse_loss: %f" % (i,sess.run(mse_loss, feed_dict=&#123;x: data, y_: label&#125;)))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # draw</span></span><br><span class="line"><span class="string">    xx, yy = np.mgrid[-1.2:1.2:.01, -0.2:2.2:.01]</span></span><br><span class="line"><span class="string">    grid = np.c_[xx.ravel(), yy.ravel()]</span></span><br><span class="line"><span class="string">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span></span><br><span class="line"><span class="string">    probs = probs.reshape(xx.shape)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">plt.scatter(data[:,0], data[:,1], c=label,</span></span><br><span class="line"><span class="string">           cmap="RdBu", vmin=-.2, vmax=1.2, edgecolor="white")</span></span><br><span class="line"><span class="string">plt.contour(xx, yy, probs, levels=[.5], cmap="Greys", vmin=0, vmax=.1)</span></span><br><span class="line"><span class="string">plt.show()</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train</span></span><br><span class="line">train_op = tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line">TRAINING_STEPS = <span class="number">40000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">        sess.run(train_op, feed_dict=&#123;x: data, y_: label&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"After %d steps, loss: %f"</span> % (i, sess.run(loss, feed_dict=&#123;x: data, y_: label&#125;)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># draw</span></span><br><span class="line">    xx, yy = np.mgrid[<span class="number">-1</span>:<span class="number">1</span>:<span class="number">.01</span>, <span class="number">0</span>:<span class="number">2</span>:<span class="number">.01</span>]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">plt.scatter(data[:,<span class="number">0</span>], data[:,<span class="number">1</span>], c=label,</span><br><span class="line">           cmap=<span class="string">"RdBu"</span>, vmin=<span class="number">-.2</span>, vmax=<span class="number">1.2</span>, edgecolor=<span class="string">"white"</span>)</span><br><span class="line">plt.contour(xx, yy, probs, levels=[<span class="number">.5</span>], cmap=<span class="string">"Greys"</span>, vmin=<span class="number">0</span>, vmax=<span class="number">.1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>这一段代码比起上两段代码就略显复杂，因此为了更好的说明正则项带来的效果通过matplotlib库进行了图形的绘制，实际上我们可以忽略图形绘制的过程直接看正则项的设置过程，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, lambda1)</span>:</span></span><br><span class="line">    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(lambda1)(var))</span><br><span class="line">    <span class="keyword">return</span> var</span><br></pre></td></tr></table></figure></p>
<p>我们看到在权重的设计过程中我们通过tf.contrib.layers.l2_regularizer(lambda1)(var)函数设置了一个$L2$正则项，通过正则项的约束可以进行求解，为了直观的看到约束结果做图如下：<br><img src="https://lh3.googleusercontent.com/-iX7_P3_QliA/WbUy1HNiJpI/AAAAAAAACV0/iFK6z4JvKhMDXPy3w3ty5JheVbpbQuSDgCLcBGAs/s0/%25E6%2597%25A0%25E6%25AD%25A3%25E5%2588%2599%25E5%258C%2596.png" alt="无正则化" title="无正则化.png"><br>上图是无正则化项的求解结果，从求解结果可以看出在无正则化的情况下出现了比较明显的过拟合现象，过拟合现象的存在极大的降低了模型的泛化性能，使得求解结果不理想．　　<br><img src="https://lh3.googleusercontent.com/-zq8OJDrp9l8/WbUzoGir_WI/AAAAAAAACWE/qaHQ1PditN4mqXBKYTeOeBfvk0P-D14AwCLcBGAs/s0/%25E6%25AD%25A3%25E5%2588%2599%25E9%25A1%25B9.png" alt="正则项" title="正则项.png"><br>上图是添加了正则项的求解结果，从求解结果上比较，添加了正则项后虽然样本的拟合误差有所增加但是模型更加简单，增加了模型的泛化性，因此添加正则项后的模型更具有普适性，具有更好的效果．<br>以上所有代码都可以再Github中找到，<a href="https://github.com/RemoteSensingFrank/tensorflow-learn.git" target="_blank" rel="noopener">传送门</a></p>
</div><div class="tags"></div><div class="post-nav"><a class="pre" href="/2017/09/10/周末在家/">周末在家</a><a class="next" href="/2017/09/07/CAD二次开发问题集中解决/">CAD二次开发问题集中解决</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'true' == true ? true : false;
var verify = 'true' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'uHNJtTxLkcGm3VRtuRGk53hb-gzGzoHsz',
  appKey:'h8s1QFO2dLQ62H0axpKB6WnD',
  placeholder:'说点啥吧...',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"><input type="hidden" name="si" value="http://www.wuweiblog.com"><input name="tn" type="hidden" value="bds"><input name="cl" type="hidden" value="3"><input name="ct" type="hidden" value="2097152"><input name="s" type="hidden" value="on"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/书评/">书评</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/图像处理/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/学习/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/小说/">小说</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/影评/">影评</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/游戏/">游戏</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/点云处理/">点云处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随感/">随感</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/ArcGIS，前端开发/" style="font-size: 15px;">ArcGIS，前端开发</a> <a href="/tags/ArcGIS环境配置/" style="font-size: 15px;">ArcGIS环境配置</a> <a href="/tags/ArcGIS开发/" style="font-size: 15px;">ArcGIS开发</a> <a href="/tags/开发/" style="font-size: 15px;">开发</a> <a href="/tags/linux-学习/" style="font-size: 15px;">linux 学习</a> <a href="/tags/学习/" style="font-size: 15px;">学习</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/图像处理数学原理/" style="font-size: 15px;">图像处理数学原理</a> <a href="/tags/系统架构-学习/" style="font-size: 15px;">系统架构 学习</a> <a href="/tags/效率/" style="font-size: 15px;">效率</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/机器学习，图像处理/" style="font-size: 15px;">机器学习，图像处理</a> <a href="/tags/tensorflow学习/" style="font-size: 15px;">tensorflow学习</a> <a href="/tags/随感/" style="font-size: 15px;">随感</a> <a href="/tags/Mary-and-Max，影评/" style="font-size: 15px;">Mary and Max，影评</a> <a href="/tags/断舍离，书评/" style="font-size: 15px;">断舍离，书评</a> <a href="/tags/沉默的大多数，书评/" style="font-size: 15px;">沉默的大多数，书评</a> <a href="/tags/呼兰河传-书评/" style="font-size: 15px;">呼兰河传,书评</a> <a href="/tags/雪中悍刀行-书评/" style="font-size: 15px;">雪中悍刀行,书评</a> <a href="/tags/将夜-书评/" style="font-size: 15px;">将夜,书评</a> <a href="/tags/中国经济2019-时间的答案/" style="font-size: 15px;">中国经济2019,时间的答案</a> <a href="/tags/潜规则-书评/" style="font-size: 15px;">潜规则,书评</a> <a href="/tags/未来简史-书评/" style="font-size: 15px;">未来简史,书评</a> <a href="/tags/熊镇-书评/" style="font-size: 15px;">熊镇,书评</a> <a href="/tags/图像处理/" style="font-size: 15px;">图像处理</a> <a href="/tags/你好疯子，影评/" style="font-size: 15px;">你好疯子，影评</a> <a href="/tags/openMVG-openMVS学习/" style="font-size: 15px;">openMVG openMVS学习</a> <a href="/tags/喜剧之王，影评/" style="font-size: 15px;">喜剧之王，影评</a> <a href="/tags/The-Legend-of-1900/" style="font-size: 15px;">The Legend of 1900</a> <a href="/tags/随感-摄影测量/" style="font-size: 15px;">随感-摄影测量</a> <a href="/tags/随感，毕业/" style="font-size: 15px;">随感，毕业</a> <a href="/tags/校正方法，控制点，光束法平差/" style="font-size: 15px;">校正方法，控制点，光束法平差</a> <a href="/tags/linux学习/" style="font-size: 15px;">linux学习</a> <a href="/tags/月亮与六便士/" style="font-size: 15px;">月亮与六便士</a> <a href="/tags/电影十二公民/" style="font-size: 15px;">电影十二公民</a> <a href="/tags/小说/" style="font-size: 15px;">小说</a> <a href="/tags/随感，死亡/" style="font-size: 15px;">随感，死亡</a> <a href="/tags/V字仇杀队-浪潮，影评/" style="font-size: 15px;">V字仇杀队,浪潮，影评</a> <a href="/tags/指环王，这个杀手不太冷/" style="font-size: 15px;">指环王，这个杀手不太冷</a> <a href="/tags/随感－代码重构/" style="font-size: 15px;">随感－代码重构</a> <a href="/tags/遗愿清单，影评/" style="font-size: 15px;">遗愿清单，影评</a> <a href="/tags/R-学习/" style="font-size: 15px;">R 学习</a> <a href="/tags/点云处理/" style="font-size: 15px;">点云处理</a> <a href="/tags/爱乐之城，影评/" style="font-size: 15px;">爱乐之城，影评</a> <a href="/tags/狗子日记/" style="font-size: 15px;">狗子日记</a> <a href="/tags/一个叫欧维的男人决定去死，书评/" style="font-size: 15px;">一个叫欧维的男人决定去死，书评</a> <a href="/tags/星际穿越，影评/" style="font-size: 15px;">星际穿越，影评</a> <a href="/tags/图像处理的数学原理/" style="font-size: 15px;">图像处理的数学原理</a> <a href="/tags/共产党宣言/" style="font-size: 15px;">共产党宣言</a> <a href="/tags/社交网络，影评/" style="font-size: 15px;">社交网络，影评</a> <a href="/tags/秒速五厘米/" style="font-size: 15px;">秒速五厘米</a> <a href="/tags/海涛之声，影评/" style="font-size: 15px;">海涛之声，影评</a> <a href="/tags/饥荒/" style="font-size: 15px;">饥荒</a> <a href="/tags/白日梦想家，影评/" style="font-size: 15px;">白日梦想家，影评</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/01/31/社会与资本/">社会与资本</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/25/有那么一瞬间/">有那么一瞬间</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/06/死亡记录/">死亡记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/06/不读书-九/">不读书(九)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/16/挣扎/">挣扎</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/02/点到三维空间直线距离计算以向量方式计算/">点到三维空间直线距离计算以向量方式计算</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/30/ArcGIS-结合-WebGL动态渲染1/">ArcGIS 结合 WebGL动态渲染1</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/30/狗子日记十一月四号/">狗子日记十一月四号</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/18/点云处理总结/">点云处理总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/05/社会青年/">社会青年</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/RemoteSensingFrank" title="Github" target="_blank">Github</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">吴蔚.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>